# Abstractive and Extractive Research Article Summarization

 Sai Vineeth K R


## Motivation
As a research student, I spend most of my time in a literature survey before diving into a solution. Most of the time I either deviate towards an off-topic or spend hours maneuvering over journals delaying the work. This motivated me to work on the Research Paper Text Summarization tool for my #NLP Class project. The aim is to optimize the time spent on research papers by academics during their research work by providing them concise and informative summaries of papers.

## Background
The main problem relating to text summarization arises from the severe lack of labeled training data. This hurdle has been overcome through this academic research paper: “A Supervised Approach to Extractive Summarisation of Scientific Papers” [1]. They give us a way to mine scientific papers and also create preliminary summaries that can be used as training data. Furthermore, they proposed multiple methods, both analytical and empirical to create text summaries using everything from scoring sentences based on word overlap to advanced methods such as Recurrent Neural Networks and LSTMs. This paper was the basis for the first half of our project, where we explored extractive methods to generate summaries.

Recurrent Neural Networks (RNNs), while not cutting edge, form the backbone of text generation today. RNNs help deal with the sequential nature of text while also accounting for the dependencies words have on each other [5]. However, they do not account for long term dependencies between words in text. Long Short Term Memory(LSTM) units overcome this by providing an attention mechanism to remember previous words in the passage. This allows for text to be generated from longer, more natural sentences and account for the contextual nature of words. This is the concept used in the second half of our project where we worked on generating summaries using abstractive methods.

The state of the art in today’s Natural Language Processing landscape is the use of Transformers namely BERT(Google) [7], GPT-2(OpenAI), et cetera. These improve upon the  by    LSTM concept of attention by using words that occur before and after a certain word. While our system does not compare with the state of the art today, they give a good idea of the differences between extractive and abstractive summarization methods. Implementing transformers is considered to be future work.


## Project
### Extractive Approach
With the data from Science Direct paper which are well labeled sectionwise, I  used a Support Vector Machine Regressor to predict sentence scores based on document vectors generated by Gensim’s doc2vec function. An additional experiment was carried out using pre-trained Glove embeddings but the results were unsatisfactory. The results were adequae, but can be improved 

#### Generated Extractive Summary
```
[' Sustained low-intensity muscle activity has been associated with adducted and extended wrist postures during keyboard intensive tasks (Dennerlein and Johnson, 2006)',
 '5, FDP=1, FDS=0N/cm2) (Brook etal',
 ' MCP adducted 3',
 ' The highest total muscle forces of three intrinsic muscles occurred in the adducted wrist posture']
```
#### Author Generated Highlights
```
['We quantified the effect of four wrist postures during tapping on resulting finger and wrist muscle stress (including both active and passive component)',
 ' Neutral wrist posture was the optimal option among four tested wrist postures when all muscles were considered',
 ' Extensor muscles exhibited higher muscle stresses than flexors',
 ' Wrist extensors stress remained higher than 4',
 '5N/cm and wrist flexor stress remained below 0'
 ]
```
#### Rogue Scores for Extractive Model
We used ROUGE scores to quantify our results. 
```
{'rouge-1': {'f': 0.2093023206246621,
   'p': 0.23076923076923078,
   'r': 0.19148936170212766},
  'rouge-2': {'f': 0.05882352453479472,
   'p': 0.06976744186046512,
   'r': 0.05084745762711865},
  'rouge-l': {'f': 0.1600446237017552,
   'p': 0.1794871794871795,
   'r': 0.14893617021276595}}
```

### Abstractive Approach
To account for Word interdependencies, I used LSTMs to encode context information. Keras also does not possess an inbuilt implementation of an Attention Layer, sourced a third-party implementation to create our model[8]. Lastly, increased the number of layers in our Neural Network to three, to capture more latent information about words.

#### Highlights
```
the process after bone loss group of 25 patients 17 females and 8 males were evaluated we used five methods for calculating fractal dimension in the region of interest fractal dimension decreases during the process after bone loss the methods of calculating fractal dimension give different results but consistent 
```
#### Predicted summary
```
 we propose a new model for the model of the model of the model we propose a new model for the model of the model we propose a new model for the model of the model we propose a new model for the model of the system topological pwr
```

#### Rogue Scores for Abstractive Model
```
{'rouge-1': {'f': 0.17532959684816446,
  'p': 0.3350513594592542,
  'r': 0.12188175690842054},
 'rouge-2': {'f': 0.030755802549186344,
  'p': 0.06458333333333334,
  'r': 0.02072526507309116},
 'rouge-l': {'f': 0.11545382682223433,
  'p': 0.29515996430470115,
  'r': 0.10673200005607746}}
```
#### Abstractive Model Loss Function
<img src="images/Abstractive%20Loss%20Function.png" width="400" height="300">


## Data to fiddle with
Google Drive links to the data: <br>
[Parsed_Papers](https://drive.google.com/drive/folders/1Xz78mg_5LLQUhumIqwQYw93apSsO3DFi) <br>
[papers.pkl](https://drive.google.com/file/d/1RPsgg32xLBUwbMr3eV0uTqGuVgUQSGzu/view)



## References

[1] [A Supervised Approach to Extractive Summarisation of Scientific Papers,
Ed Collins, Isabelle Augenstein, Sebastian Riedel, 2017 2](https://arxiv.org/pdf/1706.03946.pdf)

[2] [Data-driven Summarization of Scientific Articles - Nikola I. Nikolov, Michael Pfeiffer, Richard H.R. Hahnloser, 2018](
https://arxiv.org/pdf/1804.08875.pdf)

[3] [Text Summarization with Pretrained Encoders - Yang Liu and Mirella Lapata, 2019](https://arxiv.org/pdf/1908.08345v2.pdf)

[4] A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents - Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung BuiSeokhwan Kim, Walter Chang, Nazli Goharian, 2018

[5] Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) - Network Alex Sherstinsky, 2018

[6] Attention Is All You Need - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, 2017

[7] Leveraging BERT for Extractive Text Summarization on Lectures -  Derek Miller, 2018

[8] [Attention Layer Code](https://github.com/thushv89/attention_keras/blob/master/layers/attention.py)
